name: S2B Crawler Auto Run

on:
#  schedule:
#    - cron: '0 0 * * *'
  workflow_dispatch:

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v3
      with:
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run Crawler
      run: |
        python crawler.py

    - name: Commit Results (Excel & Error Log)
      run: |
        git config --global user.name "github-actions[bot]"
        git config --global user.email "github-actions[bot]@users.noreply.github.com"
        
        # 엑셀 파일 추가
        git add s2b_result.xlsx
        
        # 에러 HTML 파일이 있다면 추가 (원인 분석용)
        if [ -f s2b_error.html ]; then
          git add s2b_error.html
        fi
        
        git commit -m "Update S2B Data: $(date +'%Y-%m-%d')" || exit 0
        
        git push "https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git" HEAD:main