**[역할]**
너는 파이썬 웹 크롤링 전문가야. 공공기관 사이트(S2B 학교장터)의 데이터를 수집하는 견고하고 보안 회피 기능이 탑재된 크롤러를 작성해줘.

**[대상 사이트 정보]**
1. **URL:** `https://www.s2b.kr/S2BNCustomer/tcmo001.do`
2. **방식:** `POST` 요청
3. **인코딩:** `euc-kr` (한글 깨짐 방지 필수)
4. **HTML 구조:** 결과 테이블의 데이터는 **2개의 <tr>이 하나의 데이터(Item)**를 구성함.
   - 첫 번째 행: No(숫자), 계약구분, 계약번호, 계약명, 금액, 계약대상자
   - 두 번째 행: 거래구분, 기관명, 견적요청일, 계약일
   - **유효성 검사:** 첫 번째 `td`의 텍스트가 숫자(No)인 경우만 데이터로 간주해야 함.

**[핵심 기능 요구사항]**
1. **실행 파라미터 (`argparse` 사용):**
   - `date` (필수): 조회할 날짜 (YYYYMMDD). (시작일과 종료일을 이 날짜로 동일하게 설정)
   - `--page` (옵션, 기본값 1): **수집을 시작할 페이지 번호**. (중간에 끊겼을 때 이어서 하기 위함)
2. **차단 회피 (Anti-Scraping) 로직:**
   - **헤더 위장:** `User-Agent`를 포함하여 실제 브라우저와 유사한 헤더를 랜덤하게 생성하여 사용.
   - **랜덤 딜레이:** 매 페이지 요청 시 2~5초 사이의 랜덤한 시간을 대기.
   - **세션/브라우저 갱신:** **매 2페이지 요청마다** `requests.Session()`을 초기화하고 브라우저 헤더를 새로 교체하여 추적을 회피할 것.
3. **종료 조건:**
   - 해당 페이지에 데이터 테이블이 없거나, 추출된 데이터가 0건일 경우 즉시 종료.
   - **무한 루프 방지:** 서버가 마지막 페이지 이후에도 계속 데이터를 중복해서 보내는 경우가 있으므로, 이전 페이지 데이터와 비교하여 중복 시 종료.
4. **결과 저장:**
   - **파일명:** `s2b_result_{날짜}_start_p{시작페이지}.xlsx`
   - **데이터:** 수집된 모든 데이터를 엑셀로 저장.
5. **로그 기록 (중요):**
   - 엑셀 파일명과 동일한 이름의 `.log` 파일을 생성.
   - 콘솔(터미널)에 출력되는 모든 진행 상황(현재 페이지, 수집 건수, 세션 갱신 여부 등)을 로그 파일에도 실시간으로 기록(`flush` 처리).

**[코드 작성 시 주의사항]**
- `urllib3`의 SSL 경고(`InsecureRequestWarning`)를 무시하도록 설정할 것.
- `try-except` 구문을 사용하여 네트워크 통신 에러가 발생해도 크롤러가 바로 죽지 않고 로그를 남기도록 할 것.
- `pandas`, `openpyxl`, `requests`, `BeautifulSoup` 라이브러리를 사용할 것.

위 요구사항을 모두 만족하는 **완벽한 파이썬 전체 소스 코드**를 작성해줘.